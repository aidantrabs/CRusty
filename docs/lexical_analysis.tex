\documentclass[12pt]{article}
\usepackage[a4paper total={10in, 10in}]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{tgbonum}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{listings-rust}
\usepackage{textcomp}
\usepackage{datetime}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{float}

\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\pagestyle{fancy}
\fancyhf{} 
\fancyhead[L]{Lexical Analysis}
\fancyhead[C]{CP471 - Phase 1}
\fancyhead[R]{02.15.23}
\fancyfoot[C]{\thepage}

\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\definecolor{LGray}{lgray}{0.9}\definecolor{LGray}{lgray}{0.9}

\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{datavisualization}
% \pgfplotsset{compat=1.9}
% \usepgfplotslibrary{external}
% \tikzexternalize

\lstset{frame=tb,
  language=Rust,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\begin{document} {
    % ------------- COVER PAGE -------------
    \fontfamily{put}\selectfont
    \title
    { 
        \normalsize \textsc{}
        \\ [2.0cm]
        \HRule{3pt} \\
        \LARGE \textbf
        {
            {
                CP471 - Intro to Compiling \\
                Phase \#1 \\
            }
            \HRule{3pt} \\ [0.5cm]
            \textbf{\LARGE{Topic: Lexical Analysis}}
        }
    }
    
    \author {
    	Aidan Traboulay
	}
    
    \date {
        \textbf{Wednesday 15th February, 2023}
    }
    
    \maketitle
    % ------------- INTRO --------------
    \newpage
    \section {Introduction}
    
\begin{spacing}{2.00}
    The initial stage of a compiler is called lexical analysis, and it has a fundamental role in converting a high-level programming language into machine language that can be understood by computers. This stage involves examining the program's source code and breaking it down into distinct "tokens," which are the fundamental components of the programming language. These tokens are then evaluated and sorted according to their classification, including keywords, identifiers, operators, and literals. The primary objective of this phase is to transform the original source code into a well-organized set of tokens that can be effortlessly handled by the following phase of the compiler, which is the parser.
\end{spacing}
    % ------------- Grammar --------------
    \newpage
    \section {Grammar}
    \begin{lstlisting}[language={}]
<program> ::= <fdecls> <declarations> <statement_seq>.
<fdecls> ::= <fdec>; | <fdecls> <fdec>; |
<fdec> ::= def <type> <fname> ( <params> ) <declarations> <statement_seq> fed
<params> ::= <type> <var> | <type> <var> , <params> |
<fname> ::= <id>
<declarations> ::= <decl>; | <declarations> <decl>; |
<decl> := <type> <varlist>
<type> := int | double
<varlist> ::= <var>, <varlist> | <var>
<statement_seq> ::= <statement> | <statement>; <statement_seq>
<statement> ::= <var> = <expr> |
if <bexpr> then <statement_seq> fi |
if <bexpr> then <statement_seq> else <statement_seq> fi |
while <bexpr> do <statement_seq> od |
print <expr> |
return <expr> |
<expr> ::= <expr> + <term> | <expr> - <term> | <term>
<term> ::= <term> * <factor> | <term> / <factor> | <term> % <factor> |
<factor>
<factor> ::= <var> | <number> | (<expr>) | <fname>(<exprseq>)
<exprseq> ::= <expr>, <exprseq> | <expr> |
<bexpr> ::= <bexpr> or <bterm> | <bterm>
<bterm> ::= <bterm> and <bfactor> | <bfactor>
<bfactor> ::= (<bexpr>) | not <bfactor> | (<expr> <comp> <expr>)
<comp> ::= < | > | == | <= | >= | <>
<var> ::= <id> | <id>[<expr>]
<letter> ::= a | b | c | ... | z
<digit> ::= 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 0
<id> ::= <letter> | <id><letter> | <id><digit>
<number> ::= <integer> | <double>...
    \end{lstlisting}

    % ------------- Terminals & Non-Terminals --------------
    \newpage
    \section {Terminals \& Non-Terminals}
    \begin{multicols}{2}
    Non-Terminals
    \begin{lstlisting}[language={}]
<program>
<fdecls>
<fdec>
<params>
<fname>
<declarations>
<decl>
<type>
<varlist>
<statement_seq>
<statement>
<expr>
<term>
<factor>
<exprseq>
<bexpr>
<bterm>
<bfactor>
<comp>
<var>
<letter>
<digit>
<id>
<number>
    \end{lstlisting}
    \columnbreak
    Terminals
    \begin{lstlisting}[language={}]
def
fed
(
)
,
int
double
=
if
then
else
while
do
od
print
return
/
%
or
and
not
<
==
<=
=
<>
[
]
a, b, c, ..., z (lowercase)
1, 2, 3, 4, 5, 6, 7, 8, 9, 0 (digits)
    \end{lstlisting}
    \end{multicols}
    % ------------- DFA --------------
    \newpage
    \section {Deterministic Finite Automaton}
    \subsection{State Diagram}
\begin{center}
\begin{tikzpicture}[scale=0.2]
\tikzstyle{every node}+=[inner sep=0pt]
\draw [black] (6.7,-28.8) circle (2);
\draw (6.7,-28.8) node {$0$};
\draw [black] (74.7,-27.1) circle (2);
\draw (74.7,-27.1) node {$25$};
\draw (77.56,-27.1) node {$*$};
\draw [black] (74.7,-27.1) circle (1.4);
\draw [black] (6.7,-48) circle (2);
\draw (6.7,-48) node {$1$};
\draw [black] (6.7,-48) circle (1.4);
\draw [black] (12.7,-45.9) circle (2);
\draw (12.7,-45.9) node {$2$};
\draw [black] (30.7,-44.5) circle (2);
\draw (30.7,-44.5) node {$3$};
\draw [black] (19.6,-56.3) circle (2);
\draw (19.6,-56.3) node {$4$};
\draw [black] (40.9,-53.4) circle (2);
\draw (40.9,-53.4) node {$5$};
\draw [black] (28.1,-56.3) circle (2);
\draw (28.1,-56.3) node {$6$};
\draw [black] (6.7,-10) circle (2);
\draw (6.7,-10) node {$21$};
\draw [black] (8.9,-3.7) circle (2);
\draw (8.9,-3.7) node {$22$};
\draw [black] (8.9,-3.7) circle (1.4);
\draw [black] (11.6,-15.7) circle (2);
\draw (11.6,-15.7) node {$18$};
\draw [black] (16.4,-19.2) circle (2);
\draw (16.4,-19.2) node {$16$};
\draw [black] (25.2,-3.7) circle (2);
\draw (25.2,-3.7) node {$20$};
\draw [black] (25.2,-3.7) circle (1.4);
\draw [black] (40.9,-27.1) circle (2);
\draw (40.9,-27.1) node {$12$};
\draw [black] (61.4,-24.4) circle (2);
\draw (61.4,-24.4) node {$13$};
\draw [black] (61.4,-24.4) circle (1.4);
\draw [black] (66.8,-24.4) circle (2);
\draw (66.8,-24.4) node {$14$};
\draw [black] (66.8,-24.4) circle (1.4);
\draw [black] (35.3,-7.3) circle (2);
\draw (35.3,-7.3) node {$19$};
\draw [black] (35.3,-7.3) circle (1.4);
\draw [black] (35.3,-23.1) circle (2);
\draw (35.3,-23.1) node {$15$};
\draw [black] (45.9,-15.7) circle (2);
\draw (45.9,-15.7) node {$17$};
\draw [black] (45.9,-15.7) circle (1.4);
\draw [black] (25.2,-49) circle (2);
\draw (25.2,-49) node {$7$};
\draw [black] (28.8,-40.2) circle (2);
\draw (28.8,-40.2) node {$8$};
\draw [black] (36.8,-30) circle (2);
\draw (36.8,-30) node {$11$};
\draw [black] (14.8,-41.6) circle (2);
\draw (14.8,-41.6) node {$9$};
\draw [black] (22.9,-37.5) circle (2);
\draw (22.9,-37.5) node {$10$};
\draw [black] (50.9,-33.3) circle (2);
\draw (50.9,-33.3) node {$23$};
\draw [black] (50.9,-33.3) circle (1.4);
\draw [black] (16.4,-36.8) circle (2);
\draw (16.4,-36.8) node {$24$};
\draw [black] (16.4,-36.8) circle (1.4);
\draw [black] (5.668,-46.288) arc (-152.30798:-207.69202:16.973);
\fill [black] (5.67,-46.29) -- (5.74,-45.35) -- (4.85,-45.81);
\draw (3.22,-38.4) node [left] {$digit$};
\draw [black] (6.489,-49.981) arc (21.65207:-266.34793:1.5);
\draw (1.96,-52.3) node [below] {$digit$};
\fill [black] (5,-49.04) -- (4.07,-48.87) -- (4.44,-49.8);
\draw [black] (13.582,-47.686) arc (54:-234:1.5);
\draw (12.7,-50.9) node [below] {$digit$};
\fill [black] (11.82,-47.69) -- (10.94,-48.04) -- (11.75,-48.63);
\draw [black] (18.095,-57.605) arc (-57.87352:-235.00099:6.498);
\fill [black] (18.1,-57.61) -- (17.15,-57.61) -- (17.68,-58.45);
\draw (8.61,-57.03) node [left] {$+/-$};
\draw [black] (28.759,-44.98) arc (-77.27834:-93.82689:49.019);
\fill [black] (28.76,-44.98) -- (27.87,-44.67) -- (28.09,-45.64);
\draw (21.85,-46.59) node [below] {$.$};
\draw [black] (11.01,-44.834) arc (-126.54518:-194.78521:13.362);
\fill [black] (11.01,-44.83) -- (10.67,-43.96) -- (10.07,-44.76);
\draw [black] (32.527,-45.295) arc (94.2019:-193.7981:1.5);
\draw (35.95,-48.43) node [below] {$digit$};
\fill [black] (31.18,-46.43) -- (30.74,-47.27) -- (31.74,-47.19);
\draw [black] (73.535,-28.725) arc (-36.98825:-99.85866:42.139);
\fill [black] (73.54,-28.73) -- (72.65,-29.06) -- (73.45,-29.67);
\draw (57.59,-43.1) node [below] {$final$};
\draw [black] (-1,-28.8) -- (4.7,-28.8);
\draw (-1.5,-28.8) node [left] {$s$};
\fill [black] (4.7,-28.8) -- (3.9,-28.3) -- (3.9,-29.3);
\draw [black] (21.506,-55.693) arc (106.65494:88.85139:56.728);
\fill [black] (38.9,-53.32) -- (38.11,-52.81) -- (38.09,-53.81);
\draw (29.36,-53.12) node [above] {$digit$};
\draw [black] (42.691,-54.273) arc (91.75396:-196.24604:1.5);
\draw (45.99,-57.45) node [below] {$digit$};
\fill [black] (41.3,-55.35) -- (40.82,-56.17) -- (41.82,-56.14);
\draw [black] (26.868,-57.857) arc (-50.43979:-129.56021:4.739);
\fill [black] (26.87,-57.86) -- (25.93,-57.98) -- (26.57,-58.75);
\draw (23.85,-59.44) node [below] {$.$};
\draw [black] (29.712,-55.129) arc (153.73903:-134.26097:1.5);
\draw (33.25,-55.3) node [right] {$digit$};
\fill [black] (30.01,-56.87) -- (30.51,-57.67) -- (30.95,-56.77);
\draw [black] (5.524,-27.184) arc (-147.86728:-212.13272:14.635);
\fill [black] (5.52,-11.62) -- (4.68,-12.03) -- (5.52,-12.56);
\draw (2.78,-19.4) node [left] {$=$};
\draw [black] (5.431,-8.491) arc (-156.59041:-241.90864:3.439);
\fill [black] (6.97,-4.09) -- (6.03,-4.03) -- (6.5,-4.91);
\draw (4.58,-5.24) node [left] {$=$};
\draw [black] (8.231,-8.714) arc (128.68174:23.08723:42.545);
\fill [black] (73.96,-25.24) -- (74.11,-24.31) -- (73.19,-24.7);
\draw (46.89,-0) node [above] {$final$};
\draw [black] (6.241,-26.856) arc (-171.8663:-229.1499:11.127);
\fill [black] (9.98,-16.87) -- (9.05,-17.01) -- (9.7,-17.77);
\draw (6.08,-20.57) node [left] {$<$};
\draw [black] (8.12,-27.39) -- (14.98,-20.61);
\fill [black] (14.98,-20.61) -- (14.06,-20.81) -- (14.76,-21.52);
\draw (10.46,-23.52) node [above] {$>$};
\draw [black] (12.198,-13.793) arc (158.98454:103.86279:15.903);
\fill [black] (23.23,-4.06) -- (22.34,-3.76) -- (22.58,-4.73);
\draw (15.45,-7.08) node [above] {$>$};
\draw [black] (8.639,-28.308) arc (103.52814:82.16325:81.787);
\fill [black] (38.92,-26.8) -- (38.2,-26.2) -- (38.06,-27.19);
\draw (23.67,-25.6) node [above] {$\\$};
\draw [black] (42.24,-25.616) arc (134.80016:57.10268:18.387);
\fill [black] (65.18,-23.22) -- (64.78,-22.37) -- (64.24,-23.21);
\draw (53.13,-19.81) node [above] {$t$};
\draw [black] (42.798,-26.471) arc (107.18119:87.82498:49.813);
\fill [black] (59.4,-24.28) -- (58.62,-23.75) -- (58.59,-24.75);
\draw (50.71,-24.08) node [above] {$n$};
\draw [black] (13.409,-14.848) arc (114.72338:104.30837:116.592);
\fill [black] (33.36,-7.78) -- (32.46,-7.49) -- (32.71,-8.46);
\draw (22.25,-10.33) node [above] {$=$};
\draw [black] (13.014,-14.286) arc (133.50151:26.01667:38.345);
\fill [black] (73.87,-25.28) -- (73.97,-24.34) -- (73.07,-24.78);
\draw (47.37,-3.65) node [above] {$final$};
\draw [black] (8.459,-27.849) arc (117.16387:85.37893:46.27);
\fill [black] (33.31,-22.9) -- (32.55,-22.33) -- (32.47,-23.33);
\draw (19.05,-22.9) node [above] {$alpha$};
\draw [black] (36.976,-22.01) arc (121.15523:47.25083:30.348);
\fill [black] (73.28,-25.7) -- (73.03,-24.78) -- (72.35,-25.52);
\draw (56.19,-17.1) node [above] {$final$};
\draw [black] (18.284,-18.53) arc (108.6471:84.88525:62.674);
\fill [black] (43.91,-15.49) -- (43.16,-14.92) -- (43.07,-15.92);
\draw (30.67,-15.08) node [above] {$=$};
\draw [black] (74.412,-29.079) arc (-10.19492:-94.03164:29.893);
\fill [black] (74.41,-29.08) -- (73.78,-29.78) -- (74.76,-29.95);
\draw (65.69,-47.88) node [below] {$final$};
\draw [black] (74.767,-29.099) arc (0.07271:-115.92942:31.238);
\fill [black] (74.77,-29.1) -- (74.27,-29.9) -- (75.27,-29.9);
\draw (62.45,-56.11) node [below] {$final$};
\draw [black] (23.27,-49.514) arc (-80.02677:-127.82992:11.575);
\fill [black] (23.27,-49.51) -- (22.4,-49.16) -- (22.57,-50.15);
\draw (17.53,-49.94) node [below] {$\%$};
\draw [black] (73.803,-28.888) arc (-28.02421:-104.24427:41.351);
\fill [black] (73.8,-28.89) -- (72.99,-29.36) -- (73.87,-29.83);
\draw (56.31,-47.8) node [below] {$final$};
\draw [black] (18.018,-18.025) arc (124.61979:39.94637:41.532);
\fill [black] (73.45,-25.54) -- (73.32,-24.6) -- (72.56,-25.24);
\draw (47.93,-10.33) node [above] {$final$};
\draw [black] (14.59,-45.23) -- (26.91,-40.87);
\fill [black] (26.91,-40.87) -- (25.99,-40.66) -- (26.33,-41.61);
\draw (21.46,-43.57) node [below] {$/$};
\draw [black] (73.18,-28.4) arc (-50.50349:-97.6387:55.144);
\fill [black] (73.18,-28.4) -- (72.24,-28.52) -- (72.88,-29.29);
\draw (55.12,-39.5) node [below] {$final$};
\draw [black] (8.7,-28.88) -- (34.8,-29.92);
\fill [black] (34.8,-29.92) -- (34.02,-29.39) -- (33.98,-30.39);
\draw (21.89,-28.74) node [above] {$logic\mbox{ }ops$};
\draw [black] (72.752,-27.552) arc (-77.41543:-93.83341:119.259);
\fill [black] (72.75,-27.55) -- (71.86,-27.24) -- (72.08,-28.21);
\draw (56.14,-30.73) node [below] {$final$};
\draw [black] (13.224,-40.37) arc (-130.89051:-164.45721:19.711);
\fill [black] (13.22,-40.37) -- (12.95,-39.47) -- (12.29,-40.22);
\draw (8.85,-37.31) node [left] {$[/($};
\draw [black] (16.58,-40.7) -- (21.12,-38.4);
\fill [black] (21.12,-38.4) -- (20.18,-38.32) -- (20.63,-39.21);
\draw (20.28,-40.06) node [below] {$]/)$};
\draw [black] (72.959,-28.084) arc (-61.2372:-96.05786:81.935);
\fill [black] (72.96,-28.08) -- (72.02,-28.03) -- (72.5,-28.91);
\draw (50.98,-37.29) node [below] {$final$};
\draw [black] (48.952,-33.754) arc (-77.74294:-113.88356:65.518);
\fill [black] (48.95,-33.75) -- (48.06,-33.44) -- (48.28,-34.41);
\draw (28.26,-35.47) node [below] {$;$};
\draw [black] (8.24,-30.07) -- (14.86,-35.53);
\fill [black] (14.86,-35.53) -- (14.56,-34.63) -- (13.92,-35.4);
\draw (10.79,-33.29) node [below] {$.$};
\draw [black] (33.961,-21.624) arc (249.9454:-38.0546:1.5);
\draw (31.91,-18.13) node [above] {$alpha$};
\fill [black] (35.66,-21.14) -- (36.4,-20.56) -- (35.46,-20.22);
\end{tikzpicture}
\end{center}
    
    % ------------- Transition Table --------------
    \newpage
    \section {Transition Table}
    \begin{table}[H]
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l}
    \cline{1-17}
   State & 0...9 & a...z & =  & \textless{} & \textgreater{} & \textbackslash{} & {[} & {]} & (  & )  & / & . & \% & + & - & ;   &  \\ \cline{1-17}
    0     & 1     & 15    & 21 & 18          & 16             & 12               & 9   & 10  & 9  & 10 & 3 & 6 & 7  & 4 & 4 & 23* &  \\ \cline{1-17}
    1     & 1*    & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    2     & 2     & -     & -  & -           & -              & -                & -   & -   & -  & -  & 8 & 3 & -  & - & - & -   &  \\ \cline{1-17}
    3     & 3     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    4     & 5     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & 6 & -  & - & - & -   &  \\ \cline{1-17}
    5     & 5     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    6     & 6     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    7     & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    8     & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    9     & -     & -     & -  & -           & -              & -                & 10  & 10  & 10 & 10 & - & - & -  & - & - & -   &  \\ \cline{1-17}
    10    & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    11    & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    12    & -     & -     & -  & -           & -              & 13, 14           & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    13    & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    14    & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    15    & -     & 15    & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    16    & -     & -     & 17 & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    17    & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    18    & -     & -     & 19 & 20          & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    19    & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    20    & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    21    & -     & -     & 22 & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    22    & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    23    & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    24    & -     & -     & -  & -           & -              & -                & -   & -   & -  & -  & - & - & -  & - & - & -   &  \\ \cline{1-17}
    \end{tabular}
    \end{table}
    % ------------- IMPLEMENTATION --------------
    \newpage
    \section {Implementation}
    \begin{lstlisting}[language=Rust]
use std::fs::File;
use std::io::prelude::*;
use std::fmt;
use comfy_table::Table;

/*
     @Description: Enum of all possible tokens
     @Params: None
     @Returns: None
*/
#[derive(Debug, PartialEq)]
pub enum TokenTypes {
     Def,
     Type(String),
     Ident(String),
     LParen,
     RParen,
     Comma,
     Semicolon,
     Assign,
     Plus,
     PlusAssign,
     Minus,
     MinusEqual,
     Asterisk,
     AsteriskEqual,
     Divide,
     DivideEqual,
     Modulo,
     ModuloEqual,
     If,
     Then,
     Else,
     Fi,
     While,
     Do,
     Od,
     Print,
     Return,
     Eof,
     IntegerLiteral(i32),
     DoubleLiteral(f64),
     Or,
     And,
     Not,
     Less,
     Greater,
     Equal,
     LessEqual,
     GreaterEqual,
     NotEqual,
     LBracket,
     RBracket,
     Error,
}

/*
     @Description: String representation of the token types
     @Params: None
     @Returns: None
*/
impl fmt::Display for TokenTypes {
     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
          match *self {
               TokenTypes::Def => write!(f, "Def"),
               TokenTypes::Type(ref s) => write!(f, "Type({})", s),
               TokenTypes::Ident(ref s) => write!(f, "Ident({})", s),
               TokenTypes::LParen => write!(f, "LParen"),
               TokenTypes::RParen => write!(f, "RParen"),
               TokenTypes::Comma => write!(f, "Comma"),
               TokenTypes::Semicolon => write!(f, "Semicolon"),
               TokenTypes::Assign => write!(f, "Assign"),
               TokenTypes::Plus => write!(f, "Plus"),
               TokenTypes::PlusAssign => write!(f, "PlusAssign"),
               TokenTypes::Minus => write!(f, "Minus"),
               TokenTypes::MinusEqual => write!(f, "MinusEqual"),
               TokenTypes::Asterisk => write!(f, "Asterisk"),
               TokenTypes::AsteriskEqual => write!(f, "AsteriskEqual"),
               TokenTypes::Divide => write!(f, "Divide"),
               TokenTypes::DivideEqual => write!(f, "DivideEqual"),
               TokenTypes::Modulo => write!(f, "Modulo"),
               TokenTypes::ModuloEqual => write!(f, "ModuloEqual"),
               TokenTypes::If => write!(f, "If"),
               TokenTypes::Then => write!(f, "Then"),
               TokenTypes::Else => write!(f, "Else"),
               TokenTypes::Fi => write!(f, "Fi"),
               TokenTypes::While => write!(f, "While"),
               TokenTypes::Do => write!(f, "Do"),
               TokenTypes::Od => write!(f, "Od"),
               TokenTypes::Print => write!(f, "Print"),
               TokenTypes::Return => write!(f, "Return"),
               TokenTypes::Eof => write!(f, "Eof"),
               TokenTypes::IntegerLiteral(ref i) => write!(f, "IntegerLiteral({})", i),
               TokenTypes::DoubleLiteral(ref d) => write!(f, "DoubleLiteral({})", d),
               TokenTypes::Or => write!(f, "Or"),
               TokenTypes::And => write!(f, "And"),
               TokenTypes::Not => write!(f, "Not"),
               TokenTypes::Less => write!(f, "Less"),
               TokenTypes::Greater => write!(f, "Greater"),
               TokenTypes::Equal => write!(f, "Equal"),
               TokenTypes::LessEqual => write!(f, "LessEqual"),
               TokenTypes::GreaterEqual => write!(f, "GreaterEqual"),
               TokenTypes::NotEqual => write!(f, "NotEqual"),
               TokenTypes::LBracket => write!(f, "LBracket"),
               TokenTypes::RBracket => write!(f, "RBracket"),
               TokenTypes::Error => write!(f, "Error"),
          }
     }
}

/*
     @Description: Struct for tokens
     @Params: None
     @Returns: None
*/
#[derive(Debug)]
pub struct Token {
     pub token_type: TokenTypes,
     pub line_number: usize,
     pub column_number: usize,
     pub lexeme: String,
}

/* 
     @Description: Lexical Analyzer method
     @Param: input - String
     @Return: Token on different line with line number
*/
pub fn get_next_token(input: &str) -> Result<Vec<Token>, String> {
     let mut tokens = Vec::new();
     let mut chars = input.chars().peekable();
     let mut line_number = 1;
     let mut column_number = 0;

     while let Some(c) = chars.next() {
          if !(c == ' ' || c == '\t' || c == '\r') {
               column_number += 1;

               if c == '\n' {
                    column_number = 0;
               }
          }
          
          match c {
               '\n' => line_number += 1,
               ' ' | '\t' | '\r' => continue,
               'a'..='z' | 'A'..='Z' => {
                    let mut ident = String::new();
                    ident.push(c);
                    while let Some(&c) = chars.peek() {
                         match c {
                              'a'..='z' | 'A'..='Z' | '0'..='9' => {
                                   ident.push(c);
                                   chars.next();
                              }
                              _ => break,
                         }
                    }

                    let token_type = match ident.as_str() {
                         "def" => TokenTypes::Def,
                         "type" => TokenTypes::Type(ident.clone()),
                         "if" => TokenTypes::If,
                         "then" => TokenTypes::Then,
                         "else" => TokenTypes::Else,
                         "fi" => TokenTypes::Fi,
                         "while" => TokenTypes::While,
                         "do" => TokenTypes::Do,
                         "od" => TokenTypes::Od,
                         "print" => TokenTypes::Print,
                         "return" => TokenTypes::Return,
                         "or" => TokenTypes::Or,
                         "and" => TokenTypes::And,
                         "not" => TokenTypes::Not,
                         "int" => TokenTypes::Type(ident.clone()),
                         "double" => TokenTypes::Type(ident.clone()),
                         "bool" => TokenTypes::Type(ident.clone()),
                         "string" => TokenTypes::Type(ident.clone()),
                         "void" => TokenTypes::Type(ident.clone()),
                         _ => TokenTypes::Ident(ident.clone()),
                    };
                    
                    tokens.push(Token {
                         token_type,
                         line_number,
                         column_number,
                         lexeme: ident.as_str().to_string(),
                    });
               }

               '0'..='9' => {
                    let mut number = String::new();
                    number.push(c);
                    while let Some(&c) = chars.peek() {
                         match c {
                              '0'..='9' => {
                                   number.push(c);
                                   chars.next();
                              }
                              _ => break,
                         }
                    }

                    if let Some(&'.') = chars.peek() {
                         number.push('.');
                         chars.next();
                         while let Some(&c) = chars.peek() {
                              match c {
                                   '0'..='9' => {
                                        number.push(c);
                                        chars.next();
                                   }
                                   _ => break,
                              }
                         }
                         tokens.push(Token {
                              token_type: TokenTypes::DoubleLiteral(
                                   number.parse().expect("Unable to parse double"),
                              ),
                              line_number,
                              column_number,
                              lexeme: number.clone(),
                         });
                    } else {
                         tokens.push(Token {
                              token_type: TokenTypes::IntegerLiteral(
                                   number.parse().expect("Unable to parse integer"),
                              ),
                              line_number,
                              column_number,
                              lexeme: number.clone(),
                         });
                    }

                    if let Some(&c) = chars.peek() {
                         if c.is_alphabetic()  {
                              tokens.push(Token {
                                   token_type: TokenTypes::Error,
                                   line_number,
                                   column_number: column_number + 1,
                                   lexeme: String::from("".to_owned() + &c.to_string()),
                              });
                         }
                    }
               }

               '(' => tokens.push(Token {
                    token_type: TokenTypes::LParen,
                    line_number,
                    column_number,
                    lexeme: String::from("("),
               }),

               ')' => tokens.push(Token {
                    token_type: TokenTypes::RParen,
                    line_number,
                    column_number,
                    lexeme: String::from(")"),
               }),

               '[' => tokens.push(Token {
                    token_type: TokenTypes::LBracket,
                    line_number,
                    column_number,
                    lexeme: String::from("["),
               }),

               ']' => tokens.push(Token {
                    token_type: TokenTypes::RBracket,
                    line_number,
                    column_number,
                    lexeme: String::from("]"),
               }),

               ',' => tokens.push(Token {
                    token_type: TokenTypes::Comma,
                    line_number,
                    column_number,
                    lexeme: String::from(","),
               }),

               ';' => tokens.push(Token {
                    token_type: TokenTypes::Semicolon,
                    line_number,
                    column_number,
                    lexeme: String::from(";"),
               }),

               '=' => {
                    if let Some(&'=') = chars.peek() {
                         chars.next();
                         tokens.push(Token {
                              token_type: TokenTypes::Equal,
                              line_number,
                              column_number,
                              lexeme: String::from("=="),
                         });
                    } else {
                         tokens.push(Token {
                              token_type: TokenTypes::Assign,
                              line_number,
                              column_number,
                              lexeme: String::from("="),
                         });
                    }
               },

               '+' => {
                    if let Some(&'=') = chars.peek() {
                         chars.next();
                         tokens.push(Token {
                              token_type: TokenTypes::PlusAssign,
                              line_number,
                              column_number,
                              lexeme: String::from("+="),
                         });
                    } else {
                         tokens.push(Token {
                              token_type: TokenTypes::Plus,
                              line_number,
                              column_number,
                              lexeme: String::from("+"),
                         });
                    }
               }

               '-' => {
                    if let Some(&'=') = chars.peek() {
                         chars.next();
                         tokens.push(Token {
                              token_type: TokenTypes::MinusEqual,
                              line_number,
                              column_number,
                              lexeme: String::from("-="),
                         });
                    } else {
                         tokens.push(Token {
                              token_type: TokenTypes::Minus,
                              line_number,
                              column_number,
                              lexeme: String::from("-"),
                         });
                    }
               }

               '*' => {
                    if let Some(&'=') = chars.peek() {
                         chars.next();
                         tokens.push(Token {
                              token_type: TokenTypes::AsteriskEqual,
                              line_number,
                              column_number,
                              lexeme: String::from("*="),
                         });
                    } else {
                         tokens.push(Token {
                              token_type: TokenTypes::Asterisk,
                              line_number,
                              column_number,
                              lexeme: String::from("*"),
                         });
                    }
               }

               '/' => {
                    if let Some(&'=') = chars.peek() {
                         chars.next();
                         tokens.push(Token {
                              token_type: TokenTypes::DivideEqual,
                              line_number,
                              column_number,
                              lexeme: String::from("/="),
                         });
                    } else {
                         tokens.push(Token {
                              token_type: TokenTypes::Divide,
                              line_number,
                              column_number,
                              lexeme: String::from("/"),
                         });
                    }
               }

               '%' => {
                    if let Some(&'=') = chars.peek() {
                         chars.next();
                         tokens.push(Token {
                              token_type: TokenTypes::ModuloEqual,
                              line_number,
                              column_number,
                              lexeme: String::from("%="),
                         });
                    } else {
                         tokens.push(Token {
                              token_type: TokenTypes::Modulo,
                              line_number,
                              column_number,
                              lexeme: String::from("%"),
                         });
                    }
               }

               '<' => {
                    if let Some(&'=') = chars.peek() {
                         chars.next();
                         tokens.push(Token {
                              token_type: TokenTypes::LessEqual,
                              line_number,
                              column_number,
                              lexeme: String::from("<="),
                         });
                    } 

                    if let Some(&'>') = chars.peek() {
                         chars.next();
                         tokens.push(Token {
                              token_type: TokenTypes::NotEqual,
                              line_number,
                              column_number,
                              lexeme: String::from("<>"),
                         });
                    } else {
                         tokens.push(Token {
                              token_type: TokenTypes::Less,
                              line_number,
                              column_number,
                              lexeme: String::from("<"),
                         });
                    }
               }

               '>' => {
                    if let Some(&'=') = chars.peek() {
                         chars.next();
                         tokens.push(Token {
                              token_type: TokenTypes::GreaterEqual,
                              line_number,
                              column_number,
                              lexeme: String::from(">="),
                         });
                    } else {
                         tokens.push(Token {
                              token_type: TokenTypes::Greater,
                              line_number,
                              column_number,
                              lexeme: String::from(">"),
                         });
                    }
               }

               '!' => {
                    if let Some(&'=') = chars.peek() {
                         chars.next();
                         tokens.push(Token {
                              token_type: TokenTypes::NotEqual,
                              line_number,
                              column_number,
                              lexeme: String::from("!="),

                         });
                    } else {
                         tokens.push(Token {
                              token_type: TokenTypes::Not,
                              line_number,
                              column_number,
                              lexeme: String::from("!"),
                         });
                    }
               }

               '.' => tokens.push(Token {
                    token_type: TokenTypes::Eof,
                    line_number,
                    column_number,
                    lexeme: String::from("."),
               }),

               _ => {
                    tokens.push(Token {
                        token_type: TokenTypes::Error,
                        line_number,
                        column_number,
                        lexeme: String::from("".to_owned() + &c.to_string()),
                   });
              }
          }
     }
     Ok(tokens)
}

/* 
     @Description: Runs the get_next_token function on two buffers of text and writes to file
     @Params: file_name: String
     @Returns: None
*/
pub fn run_lexical_analysis(file_name: String) {
     let mut error_file = File::create("data/error.log").expect("Unable to create file");
     let mut valid_file = File::create("data/valid.log").expect("Unable to create file");

     let mut valid_table = Table::new();
     let mut error_table = Table::new();

     let mut buffer1 = String::new();
     let mut buffer2 = String::new();
     let mut file = File::open(file_name).expect("Unable to open file");

     file.read_to_string(&mut buffer2).expect("Unable to read file");
     buffer1.push_str(&buffer2);
     
     valid_table.set_header(vec!["Token Type", "Line Number", "Column Number", "Lexeme"]);
     error_table.set_header(vec!["Token Type", "Line Number", "Column Number", "Lexeme"]);
     
     match get_next_token(&buffer1) {
          Ok(tokens) => {
               for token in tokens {
                    if token.token_type == TokenTypes::Error {
                         error_table.add_row(vec![
                              token.token_type.to_string(),
                              token.line_number.to_string(),
                              token.column_number.to_string(),
                              token.lexeme.to_string(),
                         ]);
                    }

                    else {
                         valid_table.add_row(vec![
                              token.token_type.to_string(),
                              token.line_number.to_string(),
                              token.column_number.to_string(),
                              token.lexeme.to_string(),
                         ]);
                    }
               }
          }
          Err(e) => {
               writeln!(error_file, "{}", e).expect("Unable to write to file");
          }
     }   
 
     // let tokens = get_next_token(&buffer1);
     // println!("{:?}", tokens.unwrap()); 
     writeln!(valid_file, "{}", valid_table).expect("Unable to write to file");
     writeln!(error_file, "{}", error_table).expect("Unable to write to file");
}
    \end{lstlisting}
    
    % ------------- ANALYSIS --------------
    \newpage 
    \section {Analysis}
    
\begin{spacing}{2.00}
    The lexer was implemented in Rust for a numerous amount of reasons. Firstly, the language has an extremely built out robust type system. This allows ease of manipulation and linking of tokens to their given lexemes; this is showcased in the use with the definition of \lstinline{TokenTypes}, a set of values, stored in an enum. Futuremore t hese values are then linked to the \lstinline{Token} struct. Another important use case of Rust was its memory safety, it guarantees prevention of a buffer overflow, which can easily occur when passing in the target language to the lexer. It does this through its ownership and borrowing system, which enfores the safety guarantees at compile-time without compromising on performance - it took $~0.15s$ to build and compile. The biggest use case in this implementation was the pattern matching feature, allowing ease of use to match input from the grammar to its desired token, which is of particular importance in building the lexer.
\end{spacing}


    % ------------- REFERENCES --------------
    \newpage
    \begin{thebibliography}{5}
    \bibitem{CS143}CS143 Lecture 3 - Stanford University. (n.d.). Retrieved February 24, 2023, from \emph{https://web.stanford.edu/class/cs143/lectures/lecture03.pdf}
    \bibitem{Compiling}{Aho, A. V. (2008). Compilers : Principles, Techniques, & Tools. Greg Tobin.}
    \end{thebibliography}
\end{document}